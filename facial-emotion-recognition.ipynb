{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing the library:\n\nimport math\nimport scikitplot\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\n\nfrom matplotlib import pyplot\nfrom keras.utils import np_utils\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-07-13T06:08:01.149185Z","iopub.execute_input":"2023-07-13T06:08:01.149980Z","iopub.status.idle":"2023-07-13T06:08:08.718264Z","shell.execute_reply.started":"2023-07-13T06:08:01.149931Z","shell.execute_reply":"2023-07-13T06:08:08.717428Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the data:\n\ndf = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:08:08.720141Z","iopub.execute_input":"2023-07-13T06:08:08.720654Z","iopub.status.idle":"2023-07-13T06:08:14.906710Z","shell.execute_reply.started":"2023-07-13T06:08:08.720608Z","shell.execute_reply":"2023-07-13T06:08:14.905862Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Displaying the data:\n\ndf","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:08:14.908214Z","iopub.execute_input":"2023-07-13T06:08:14.908681Z","iopub.status.idle":"2023-07-13T06:08:14.935173Z","shell.execute_reply.started":"2023-07-13T06:08:14.908641Z","shell.execute_reply":"2023-07-13T06:08:14.934404Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       emotion                                             pixels        Usage\n0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n...        ...                                                ...          ...\n35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n\n[35887 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emotion</th>\n      <th>pixels</th>\n      <th>Usage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>35882</th>\n      <td>6</td>\n      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n      <td>PrivateTest</td>\n    </tr>\n    <tr>\n      <th>35883</th>\n      <td>3</td>\n      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n      <td>PrivateTest</td>\n    </tr>\n    <tr>\n      <th>35884</th>\n      <td>0</td>\n      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n      <td>PrivateTest</td>\n    </tr>\n    <tr>\n      <th>35885</th>\n      <td>3</td>\n      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n      <td>PrivateTest</td>\n    </tr>\n    <tr>\n      <th>35886</th>\n      <td>2</td>\n      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n      <td>PrivateTest</td>\n    </tr>\n  </tbody>\n</table>\n<p>35887 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Giving the the label:\n\nemotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:08:14.937073Z","iopub.execute_input":"2023-07-13T06:08:14.937385Z","iopub.status.idle":"2023-07-13T06:08:14.942516Z","shell.execute_reply.started":"2023-07-13T06:08:14.937343Z","shell.execute_reply":"2023-07-13T06:08:14.941777Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Determine the dimensions of an image represented by pixel values:\n\nmath.sqrt(len(df.pixels[0].split(' ')))","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:08:14.945959Z","iopub.execute_input":"2023-07-13T06:08:14.946408Z","iopub.status.idle":"2023-07-13T06:08:14.958130Z","shell.execute_reply.started":"2023-07-13T06:08:14.946371Z","shell.execute_reply":"2023-07-13T06:08:14.957392Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"48.0"},"metadata":{}}]},{"cell_type":"code","source":"# Resizing the image to make it compitable with CNN model:\n\nimg_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nimg_array = np.stack(img_array, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:08:14.961091Z","iopub.execute_input":"2023-07-13T06:08:14.961397Z","iopub.status.idle":"2023-07-13T06:09:33.115108Z","shell.execute_reply.started":"2023-07-13T06:08:14.961362Z","shell.execute_reply":"2023-07-13T06:09:33.114158Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Displaying the image shape:\n\nimg_array.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.116538Z","iopub.execute_input":"2023-07-13T06:09:33.116824Z","iopub.status.idle":"2023-07-13T06:09:33.121945Z","shell.execute_reply.started":"2023-07-13T06:09:33.116784Z","shell.execute_reply":"2023-07-13T06:09:33.121148Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(35887, 48, 48, 1)"},"metadata":{}}]},{"cell_type":"code","source":"# Encoding the labels:\n\nle = LabelEncoder()\nimg_labels = le.fit_transform(df.emotion)\nimg_labels = np_utils.to_categorical(img_labels)\nimg_labels.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.123305Z","iopub.execute_input":"2023-07-13T06:09:33.123798Z","iopub.status.idle":"2023-07-13T06:09:33.138983Z","shell.execute_reply.started":"2023-07-13T06:09:33.123749Z","shell.execute_reply":"2023-07-13T06:09:33.138156Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(35887, 7)"},"metadata":{}}]},{"cell_type":"code","source":"le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(le_name_mapping)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.140664Z","iopub.execute_input":"2023-07-13T06:09:33.140886Z","iopub.status.idle":"2023-07-13T06:09:33.147037Z","shell.execute_reply.started":"2023-07-13T06:09:33.140861Z","shell.execute_reply":"2023-07-13T06:09:33.146246Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Splitting data into training and validation set:\n\nX_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,\n                                                    shuffle=True, stratify=img_labels,\n                                                    test_size=0.1, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.148330Z","iopub.execute_input":"2023-07-13T06:09:33.148797Z","iopub.status.idle":"2023-07-13T06:09:33.630777Z","shell.execute_reply.started":"2023-07-13T06:09:33.148759Z","shell.execute_reply":"2023-07-13T06:09:33.629957Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((32298, 48, 48, 1), (3589, 48, 48, 1), (32298, 7), (3589, 7))"},"metadata":{}}]},{"cell_type":"code","source":"# Assigning values:\n\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]\nimg_depth = X_train.shape[3]\nnum_classes = y_train.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.632148Z","iopub.execute_input":"2023-07-13T06:09:33.632620Z","iopub.status.idle":"2023-07-13T06:09:33.638164Z","shell.execute_reply.started":"2023-07-13T06:09:33.632579Z","shell.execute_reply":"2023-07-13T06:09:33.637452Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Normalizing results, as neural networks are very sensitive to unnormalized data:\n\nX_train = X_train / 255.\nX_valid = X_valid / 255.","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.639523Z","iopub.execute_input":"2023-07-13T06:09:33.640033Z","iopub.status.idle":"2023-07-13T06:09:33.742129Z","shell.execute_reply.started":"2023-07-13T06:09:33.639985Z","shell.execute_reply":"2023-07-13T06:09:33.741247Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Creating a custom CNN Model \n\n\ndef build_model(optim):\n    net = Sequential(name='CNN')\n\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(5,5),\n            input_shape=(img_width, img_height, img_depth),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_1'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_1'))\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(5,5),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_2'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_2'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n    net.add(Dropout(0.4, name='dropout_1'))\n\n    net.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_3'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_3'))\n    net.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_4'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_4'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n    net.add(Dropout(0.4, name='dropout_2'))\n\n    net.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_5'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_5'))\n    net.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_6'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_6'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n    net.add(Dropout(0.5, name='dropout_3'))\n\n    net.add(Flatten(name='flatten'))\n        \n    net.add(\n        Dense(\n            128,\n            activation='elu',\n            kernel_initializer='he_normal',\n            name='dense_1'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_7'))\n    \n    net.add(Dropout(0.6, name='dropout_4'))\n    \n    net.add(\n        Dense(\n            num_classes,\n            activation='softmax',\n            name='out_layer'\n        )\n    )\n    \n    net.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n    )\n    \n    net.summary()\n    \n    return net","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.743778Z","iopub.execute_input":"2023-07-13T06:09:33.744075Z","iopub.status.idle":"2023-07-13T06:09:33.766823Z","shell.execute_reply.started":"2023-07-13T06:09:33.744039Z","shell.execute_reply":"2023-07-13T06:09:33.765565Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# To avoid overfitting, early stopping and learning rate are added:\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.00005,\n    patience=11,\n    verbose=1,\n    restore_best_weights=True,\n)  # early stopping\n\nlr_scheduler = ReduceLROnPlateau(   \n    monitor='val_accuracy',\n    factor=0.5,\n    patience=7,\n    min_lr=1e-7,\n    verbose=1,\n)   # learning rate\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.768374Z","iopub.execute_input":"2023-07-13T06:09:33.768907Z","iopub.status.idle":"2023-07-13T06:09:33.779382Z","shell.execute_reply.started":"2023-07-13T06:09:33.768846Z","shell.execute_reply":"2023-07-13T06:09:33.778648Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n)\ntrain_datagen.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.781021Z","iopub.execute_input":"2023-07-13T06:09:33.781328Z","iopub.status.idle":"2023-07-13T06:09:33.849800Z","shell.execute_reply.started":"2023-07-13T06:09:33.781279Z","shell.execute_reply":"2023-07-13T06:09:33.849059Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"batch_size = 32  # batch size of 32 performs the best.\n\nepochs = 100\n\noptims = [\n    optimizers.Nadam(learning_rate=0.001, \n                     beta_1=0.9, \n                     beta_2=0.999, \n                     epsilon=1e-07, \n                     name='Nadam'),\n    optimizers.Adam(0.001),\n]\n\nmodel = build_model(optims[1]) \n\nhistory = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_valid, y_valid),\n    steps_per_epoch=len(X_train) / batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)  # training the model","metadata":{"execution":{"iopub.status.busy":"2023-07-13T06:09:33.851176Z","iopub.execute_input":"2023-07-13T06:09:33.851485Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"CNN\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 48, 48, 64)        1664      \n_________________________________________________________________\nbatchnorm_1 (BatchNormalizat (None, 48, 48, 64)        256       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 48, 48, 64)        102464    \n_________________________________________________________________\nbatchnorm_2 (BatchNormalizat (None, 48, 48, 64)        256       \n_________________________________________________________________\nmaxpool2d_1 (MaxPooling2D)   (None, 24, 24, 64)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 24, 24, 64)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     \n_________________________________________________________________\nbatchnorm_3 (BatchNormalizat (None, 24, 24, 128)       512       \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 24, 24, 128)       147584    \n_________________________________________________________________\nbatchnorm_4 (BatchNormalizat (None, 24, 24, 128)       512       \n_________________________________________________________________\nmaxpool2d_2 (MaxPooling2D)   (None, 12, 12, 128)       0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 12, 12, 128)       0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 12, 12, 256)       295168    \n_________________________________________________________________\nbatchnorm_5 (BatchNormalizat (None, 12, 12, 256)       1024      \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 12, 12, 256)       590080    \n_________________________________________________________________\nbatchnorm_6 (BatchNormalizat (None, 12, 12, 256)       1024      \n_________________________________________________________________\nmaxpool2d_3 (MaxPooling2D)   (None, 6, 6, 256)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 6, 6, 256)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 9216)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               1179776   \n_________________________________________________________________\nbatchnorm_7 (BatchNormalizat (None, 128)               512       \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 128)               0         \n_________________________________________________________________\nout_layer (Dense)            (None, 7)                 903       \n=================================================================\nTotal params: 2,395,591\nTrainable params: 2,393,543\nNon-trainable params: 2,048\n_________________________________________________________________\nTrain for 1009.3125 steps, validate on 3589 samples\nEpoch 1/100\n1010/1009 [==============================] - 25s 24ms/step - loss: 2.0351 - accuracy: 0.2398 - val_loss: 1.6574 - val_accuracy: 0.3391\nEpoch 2/100\n1010/1009 [==============================] - 19s 19ms/step - loss: 1.6657 - accuracy: 0.3398 - val_loss: 1.4729 - val_accuracy: 0.4338\nEpoch 3/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.4916 - accuracy: 0.4212 - val_loss: 1.3700 - val_accuracy: 0.4723\nEpoch 4/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.3828 - accuracy: 0.4732 - val_loss: 1.3053 - val_accuracy: 0.5040\nEpoch 5/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 1.3158 - accuracy: 0.5050 - val_loss: 1.2771 - val_accuracy: 0.5235\nEpoch 6/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.2751 - accuracy: 0.5208 - val_loss: 1.1321 - val_accuracy: 0.5729\nEpoch 7/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.2386 - accuracy: 0.5342 - val_loss: 1.1187 - val_accuracy: 0.5765\nEpoch 8/100\n1010/1009 [==============================] - 19s 19ms/step - loss: 1.2111 - accuracy: 0.5457 - val_loss: 1.0678 - val_accuracy: 0.5979\nEpoch 9/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.1831 - accuracy: 0.5572 - val_loss: 1.0659 - val_accuracy: 0.5952\nEpoch 10/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.1648 - accuracy: 0.5645 - val_loss: 1.1309 - val_accuracy: 0.6110\nEpoch 11/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.1536 - accuracy: 0.5707 - val_loss: 0.9915 - val_accuracy: 0.6328\nEpoch 12/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.1384 - accuracy: 0.5738 - val_loss: 1.1366 - val_accuracy: 0.5829\nEpoch 13/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 1.1251 - accuracy: 0.5806 - val_loss: 1.0318 - val_accuracy: 0.6127\nEpoch 14/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.1129 - accuracy: 0.5827 - val_loss: 1.0001 - val_accuracy: 0.6311\nEpoch 15/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0959 - accuracy: 0.5924 - val_loss: 0.9812 - val_accuracy: 0.6305\nEpoch 16/100\n1010/1009 [==============================] - 19s 19ms/step - loss: 1.0886 - accuracy: 0.5978 - val_loss: 1.0136 - val_accuracy: 0.6250\nEpoch 17/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0874 - accuracy: 0.5979 - val_loss: 0.9979 - val_accuracy: 0.6278\nEpoch 18/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0694 - accuracy: 0.6019 - val_loss: 0.9665 - val_accuracy: 0.6453\nEpoch 19/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 1.0651 - accuracy: 0.6051 - val_loss: 0.9830 - val_accuracy: 0.6356\nEpoch 20/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0638 - accuracy: 0.6051 - val_loss: 0.9388 - val_accuracy: 0.6573\nEpoch 21/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 1.0511 - accuracy: 0.6111 - val_loss: 0.9760 - val_accuracy: 0.6461\nEpoch 22/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0456 - accuracy: 0.6147 - val_loss: 0.9617 - val_accuracy: 0.6514\nEpoch 23/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0341 - accuracy: 0.6186 - val_loss: 0.9621 - val_accuracy: 0.6503\nEpoch 24/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 1.0316 - accuracy: 0.6132 - val_loss: 0.9834 - val_accuracy: 0.6417\nEpoch 25/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0266 - accuracy: 0.6230 - val_loss: 0.9262 - val_accuracy: 0.6528\nEpoch 26/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0215 - accuracy: 0.6217 - val_loss: 0.9899 - val_accuracy: 0.6300\nEpoch 27/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0118 - accuracy: 0.6269 - val_loss: 0.9173 - val_accuracy: 0.6651\nEpoch 28/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0057 - accuracy: 0.6299 - val_loss: 0.9203 - val_accuracy: 0.6578\nEpoch 29/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 1.0078 - accuracy: 0.6271 - val_loss: 0.9482 - val_accuracy: 0.6459\nEpoch 30/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 0.9971 - accuracy: 0.6333 - val_loss: 0.9536 - val_accuracy: 0.6470\nEpoch 31/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9986 - accuracy: 0.6316 - val_loss: 0.9225 - val_accuracy: 0.6601\nEpoch 32/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 0.9902 - accuracy: 0.6329 - val_loss: 0.9480 - val_accuracy: 0.6562\nEpoch 33/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9878 - accuracy: 0.6355 - val_loss: 0.9468 - val_accuracy: 0.6512\nEpoch 34/100\n1007/1009 [============================>.] - ETA: 0s - loss: 0.9817 - accuracy: 0.6395\nEpoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9818 - accuracy: 0.6396 - val_loss: 0.9393 - val_accuracy: 0.6565\nEpoch 35/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 0.9582 - accuracy: 0.6474 - val_loss: 0.8836 - val_accuracy: 0.6723\nEpoch 36/100\n1010/1009 [==============================] - 21s 20ms/step - loss: 0.9468 - accuracy: 0.6520 - val_loss: 0.9029 - val_accuracy: 0.6701\nEpoch 37/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9472 - accuracy: 0.6496 - val_loss: 0.9001 - val_accuracy: 0.6796\nEpoch 38/100\n1010/1009 [==============================] - 19s 19ms/step - loss: 0.9383 - accuracy: 0.6586 - val_loss: 0.9163 - val_accuracy: 0.6684\nEpoch 39/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9360 - accuracy: 0.6575 - val_loss: 0.8849 - val_accuracy: 0.6734\nEpoch 40/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 0.9260 - accuracy: 0.6609 - val_loss: 0.8781 - val_accuracy: 0.6768\nEpoch 41/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9234 - accuracy: 0.6581 - val_loss: 0.8877 - val_accuracy: 0.6807\nEpoch 42/100\n1010/1009 [==============================] - 21s 20ms/step - loss: 0.9145 - accuracy: 0.6626 - val_loss: 0.8954 - val_accuracy: 0.6810\nEpoch 43/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9145 - accuracy: 0.6631 - val_loss: 0.8878 - val_accuracy: 0.6812\nEpoch 44/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9142 - accuracy: 0.6647 - val_loss: 0.9131 - val_accuracy: 0.6732\nEpoch 45/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9128 - accuracy: 0.6633 - val_loss: 0.8743 - val_accuracy: 0.6829\nEpoch 46/100\n1010/1009 [==============================] - 20s 19ms/step - loss: 0.9079 - accuracy: 0.6666 - val_loss: 0.8912 - val_accuracy: 0.6799\nEpoch 47/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.9058 - accuracy: 0.6687 - val_loss: 0.8774 - val_accuracy: 0.6846\nEpoch 48/100\n1010/1009 [==============================] - 21s 20ms/step - loss: 0.9043 - accuracy: 0.6651 - val_loss: 0.9124 - val_accuracy: 0.6709\nEpoch 49/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.8985 - accuracy: 0.6691 - val_loss: 0.8918 - val_accuracy: 0.6743\nEpoch 50/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.8975 - accuracy: 0.6695 - val_loss: 0.8919 - val_accuracy: 0.6824\nEpoch 51/100\n1010/1009 [==============================] - 20s 20ms/step - loss: 0.8975 - accuracy: 0.6707 - val_loss: 0.9004 - val_accuracy: 0.6815\nEpoch 52/100\n 658/1009 [==================>...........] - ETA: 6s - loss: 0.8890 - accuracy: 0.6764","output_type":"stream"}]},{"cell_type":"code","source":"# Plotting the Accuracy vs Val Accuracy graph:\n\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy vs Val Accuracy graph')\npyplot.tight_layout()\n\npyplot.savefig('accuracy_plot.png')  # saving the graph \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the loss vs Val loss graph: \n\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('loss_plot.png')  # saving the graph \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting confusion matrix:\n\nyhat_valid = model.predict_classes(X_valid)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\n\npyplot.savefig(\"confusion_matrix_dcnn.png\")  # saving the plot\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying wrong prediction:\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the classificiation report:\n\n\nprint(classification_report(np.argmax(y_valid, axis=1), yhat_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix clearly demonstrates that while our model performs well for the class \"happy,\" it performs poorly for other classes. The fact that the remaining classes have less data could be one of the causes. I discovered, however, that some of the photographs from these two classes make it difficult for a human to determine whether the subject is dejected or indifferent. Individual differences can be seen in facial expression. The neutral face of some people conveys sadness.","metadata":{}},{"cell_type":"code","source":"np.random.seed(2)\nrandom_sad_imgs = np.random.choice(np.where(y_valid[:, 1] == 1)[0], size=9)\nrandom_neutral_imgs = np.random.choice(np.where(y_valid[:, 2] == 1)[0], size=9)\n\nfig = pyplot.figure(1, (18, 4))\n\nfor i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n    ax = pyplot.subplot(2, 9, i + 1)\n    sample_img = X_valid[sadidx, :, :, 0]\n    true_label = \"sad\"\n    predicted_label = emotion_label_to_text[model.predict_classes(sample_img.reshape(1, 48, 48, 1))[0]]\n    if true_label == predicted_label:\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"true:{true_label}, pred:{predicted_label}\")\n        ax.spines['top'].set_color('green')\n        ax.spines['bottom'].set_color('green')\n        ax.spines['left'].set_color('green')\n        ax.spines['right'].set_color('green')\n    else:\n        ax.axis('off')\n\n    ax = pyplot.subplot(2, 9, i + 10)\n    sample_img = X_valid[neuidx, :, :, 0]\n    true_label = \"neutral\"\n    predicted_label = emotion_label_to_text[model.predict_classes(sample_img.reshape(1, 48, 48, 1))[0]]\n    if true_label == predicted_label:\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"true:{true_label}, pred:{predicted_label}\")\n        ax.spines['top'].set_color('green')\n        ax.spines['bottom'].set_color('green')\n        ax.spines['left'].set_color('green')\n        ax.spines['right'].set_color('green')\n    else:\n        ax.axis('off')\n\npyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the incorrect prediction:\n\nnp.random.seed(2)\nrandom_sad_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)\nrandom_neutral_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)\n\nfig = pyplot.figure(1, (18, 4))\n\nfor i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n        ax = pyplot.subplot(2, 9, i+1)\n        sample_img = X_valid[sadidx,:,:,0]\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"true:sad, pred:{emotion_label_to_text[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}\")\n\n        ax = pyplot.subplot(2, 9, i+10)\n        sample_img = X_valid[neuidx,:,:,0]\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:neut, p:{emotion_label_to_text[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}\")\n\n        pyplot.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the model for later use or deployment:\n\nmodel_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save(\"model.h5\")  # model save in tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:","metadata":{}},{"cell_type":"markdown","source":"- In conclusion, the model underwent numerous iterations to attain validation accuracy ranging from 73% to 80%. Even with the usage of a data generator, the model was only able to reach a certain level of accuracy due to the dataset's label imbalance. The amount of data in the dataset could be increased using data augmentation approaches to address this, which might enhance model performance while being trained.\n\n- Convolutional neural networks (CNNs) created specifically for our application utilizing the Keras Sequential API make up the model we employed. It has a number of convolutional layers as well as layers for batch normalization, max pooling, dropout, and dense layers.\n\n- The model's architecture, weights, and optimizer state were all saved in an H5 file, which was created using TensorFlow. The model's configuration was also stored in a YAML file, which contained details on the network architecture, layer settings, and training parameters. This approach for saving the model makes it simple to load and reuse it in subsequent sessions.","metadata":{}}]}